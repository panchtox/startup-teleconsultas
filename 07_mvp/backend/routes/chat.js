const OpenAI = require('openai');
const { calculateROI, generateROISummary } = require('../utils/roiCalculations');

// Cliente configurado para GitHub Models
const client = new OpenAI({
  baseURL: 'https://models.inference.ai.azure.com',
  apiKey: process.env.GITHUB_TOKEN
});

// System prompt con contexto de TeleAssist
const SYSTEM_PROMPT = `Eres un asistente virtual de TeleAssist, una plataforma B2B SaaS que reduce el ausentismo en teleconsultas del 33% a menos del 10%.

**Contexto de TeleAssist:**
- Recordatorios inteligentes por WhatsApp (48hs, 24hs, 2hs antes)
- Sistema de reputaci칩n gamificado (5 niveles: Bronce, Plata, Oro, Platino, Diamante)
- 12 badges de logros para pacientes
- Analytics en tiempo real con c치lculo de ROI
- Integraci칩n plug & play con plataformas de telemedicina existentes

**Datos actuales del sistema:**
- {{patientCount}} pacientes activos
- {{appointmentCount}} consultas programadas
- {{reductionRate}}% reducci칩n de ausentismo lograda

**Tu rol:**
- Responder preguntas sobre la plataforma
- Sugerir acciones espec칤ficas (ej: "crear consulta", "ver pacientes con score bajo")
- Explicar m칠tricas y reportes
- Ayudar con el uso de las funcionalidades

**FORMATO DE RESPUESTAS:**
- NO uses f칩rmulas LaTeX (\[ \], $, etc.)
- Escribe f칩rmulas en texto simple: "ROI = (Beneficio Neto / Costo) 칑 100"
- Usa emojis para hacer las respuestas m치s amigables: 游눯游늵游늳
- Usa saltos de l칤nea y bullets para claridad
- Usa negritas con **texto** para destacar n칰meros importantes

**CR칈TICO - C츼LCULOS DE ROI:**
Cuando el usuario mencione CUALQUIERA de estos t칠rminos:
- "ahorr", "cu치nto recupero", "cu치nto gano", "vale la pena", "ROI", "retorno", "inversi칩n", "beneficio econ칩mico", "cu치nto me implica", "pacientes que perder칤a"

DEBES OBLIGATORIAMENTE llamar a la funci칩n calculate_roi. NO intentes hacer ning칰n c치lculo manual.

Datos necesarios para calculate_roi:
- monthlyAppointments: Consultas mensuales totales
- revenuePerAppointment: Ingreso por consulta
- costPerAppointment: Costo m칠dico por consulta

Si el usuario menciona el "plan PRO" de TeleAssist, la funci칩n ya calcula autom치ticamente el costo correcto basado en el n칰mero de pacientes.

NUNCA, bajo ninguna circunstancia, hagas c치lculos de ROI manualmente. Tu rol es SOLO llamar la funci칩n y explicar los resultados que ella devuelve.

S칠 conciso, profesional y enfocado en ayudar al usuario a usar TeleAssist eficientemente.`;

// Definici칩n de funciones disponibles para el LLM
const FUNCTIONS = [
  {
    name: 'calculate_roi',
    description: 'OBLIGATORIO: Usa esta funci칩n SIEMPRE que el usuario pregunte sobre dinero, ahorros, ROI, recuperaci칩n de pacientes, cu치nto gana, cu치nto vale la pena, beneficios econ칩micos, o cu치nto implica econ칩micamente. Esta funci칩n calcula TODO: pacientes recuperados, ingresos adicionales, costo de TeleAssist, ROI neto, m칰ltiplo de retorno. NO hagas c치lculos manuales. La funci칩n calcula autom치ticamente el costo de TeleAssist seg칰n el n칰mero de pacientes.',
    parameters: {
      type: 'object',
      properties: {
        monthlyAppointments: {
          type: 'number',
          description: 'N칰mero total de consultas mensuales programadas (incluye las que asisten y las que no asisten)'
        },
        revenuePerAppointment: {
          type: 'number',
          description: 'Ingreso que recibe el cliente por cada consulta realizada (en d칩lares o pesos)'
        },
        costPerAppointment: {
          type: 'number',
          description: 'Costo que el cliente paga al m칠dico por cada consulta (honorarios m칠dicos en d칩lares o pesos)'
        },
        currentNoShowRate: {
          type: 'number',
          description: 'Tasa actual de ausentismo como decimal (ej: 0.33 para 33%). Si no se proporciona, usa 0.33 (promedio de la industria)',
          default: 0.33
        },
        targetNoShowRate: {
          type: 'number',
          description: 'Tasa objetivo de ausentismo con TeleAssist como decimal (ej: 0.08 para 8%). Si no se proporciona, usa 0.08',
          default: 0.08
        }
      },
      required: ['monthlyAppointments', 'revenuePerAppointment', 'costPerAppointment']
    }
  }
];

/**
 * Ejecuta la funci칩n solicitada por el LLM
 */
function executeFunctionCall(functionName, functionArgs) {
  if (functionName === 'calculate_roi') {
    try {
      const roiData = calculateROI(functionArgs);
      const summary = generateROISummary(roiData);
      
      // Devolver tanto el objeto estructurado como el resumen
      return JSON.stringify({
        success: true,
        data: roiData,
        summary: summary
      });
    } catch (error) {
      return JSON.stringify({
        success: false,
        error: error.message
      });
    }
  }
  
  return JSON.stringify({ success: false, error: 'Funci칩n no reconocida' });
}

/**
 * Handler del endpoint /api/chat
 * Recibe mensajes del usuario y devuelve respuestas contextuales sobre TeleAssist
 */
async function handleChat(req, res) {
  try {
    const { message, conversationHistory = [] } = req.body;

    if (!message || typeof message !== 'string') {
      return res.status(400).json({ 
        error: 'Message is required and must be a string' 
      });
    }

    // Validar que tenemos el token de GitHub
    if (!process.env.GITHUB_TOKEN) {
      return res.status(500).json({ 
        error: 'GitHub token not configured' 
      });
    }

    // Preparar el contexto con datos actuales
    // TODO: En producci칩n, estos datos vendr칤an de la base de datos
    const contextData = {
      patientCount: 500,
      appointmentCount: 520,
      reductionRate: 73
    };

    // Inyectar datos en el system prompt
    let systemPrompt = SYSTEM_PROMPT;
    Object.entries(contextData).forEach(([key, value]) => {
      systemPrompt = systemPrompt.replace(`{{${key}}}`, value);
    });

    // Construir el array de mensajes para la API
    const messages = [
      { role: 'system', content: systemPrompt },
      ...conversationHistory.slice(-10), // 칔ltimos 10 mensajes para no exceder l칤mites
      { role: 'user', content: message }
    ];

    // Llamada inicial a GitHub Models con function calling
    let response = await client.chat.completions.create({
      model: 'gpt-4o-mini', // Modelo High tier: 10 RPM, 50 RPD
      messages: messages,
      functions: FUNCTIONS, // Funciones disponibles
      function_call: 'auto', // El modelo decide cu치ndo llamar funciones
      temperature: 0.7,
      max_tokens: 800, // Aumentado para respuestas con ROI
      top_p: 0.9
    });

    let assistantMessage = response.choices[0].message;
    let totalUsage = response.usage;

    // Si el LLM quiere llamar a una funci칩n
    if (assistantMessage.function_call) {
      const functionName = assistantMessage.function_call.name;
      const functionArgs = JSON.parse(assistantMessage.function_call.arguments);
      
      console.log(`游댢 LLM llamando funci칩n: ${functionName}`, functionArgs);
      
      // Ejecutar la funci칩n
      const functionResult = executeFunctionCall(functionName, functionArgs);
      
      // Agregar el resultado de la funci칩n a la conversaci칩n
      messages.push(assistantMessage); // Mensaje del assistant con function_call
      messages.push({
        role: 'function',
        name: functionName,
        content: functionResult
      });
      
      // Segunda llamada al LLM para que interprete el resultado
      response = await client.chat.completions.create({
        model: 'gpt-4o-mini',
        messages: messages,
        temperature: 0.7,
        max_tokens: 800,
        top_p: 0.9
      });
      
      assistantMessage = response.choices[0].message;
      
      // Sumar usage de ambas llamadas
      totalUsage = {
        prompt_tokens: totalUsage.prompt_tokens + response.usage.prompt_tokens,
        completion_tokens: totalUsage.completion_tokens + response.usage.completion_tokens,
        total_tokens: totalUsage.total_tokens + response.usage.total_tokens
      };
    }

    return res.json({
      message: assistantMessage.content,
      model: 'gpt-4o-mini',
      functionCalled: !!assistantMessage.function_call,
      usage: {
        promptTokens: totalUsage.prompt_tokens,
        completionTokens: totalUsage.completion_tokens,
        totalTokens: totalUsage.total_tokens
      }
    });

  } catch (error) {
    console.error('Error in chat endpoint:', error);

    // Manejo espec칤fico de rate limiting
    if (error.status === 429) {
      return res.status(429).json({ 
        error: 'Rate limit exceeded. Please try again later.',
        retryAfter: error.headers?.['retry-after'] || 60
      });
    }

    // Error gen칠rico
    return res.status(500).json({ 
      error: 'Failed to process chat message',
      details: error.message 
    });
  }
}

/**
 * Health check del servicio de chat
 */
async function healthCheck(req, res) {
  try {
    const hasToken = !!process.env.GITHUB_TOKEN;
    
    return res.json({
      status: 'ok',
      chatEnabled: hasToken,
      model: 'gpt-4o-mini',
      provider: 'GitHub Models',
      functionsAvailable: FUNCTIONS.map(f => f.name)
    });
  } catch (error) {
    return res.status(500).json({ 
      status: 'error',
      error: error.message 
    });
  }
}

module.exports = {
  handleChat,
  healthCheck
};
