const OpenAI = require('openai');
const { calculateROI, generateROISummary } = require('../utils/roiCalculations');

// Cliente configurado para GitHub Models
const client = new OpenAI({
  baseURL: 'https://models.inference.ai.azure.com',
  apiKey: process.env.GITHUB_TOKEN
});

// System prompt con contexto de TeleAssist
const SYSTEM_PROMPT = `Eres un asistente virtual de TeleAssist, una plataforma B2B SaaS que reduce el ausentismo en teleconsultas del 33% a menos del 10%.

**Contexto de TeleAssist:**
- Recordatorios inteligentes por WhatsApp (48hs, 24hs, 2hs antes)
- Sistema de reputaci贸n gamificado (5 niveles: Bronce, Plata, Oro, Platino, Diamante)
- 12 badges de logros para pacientes
- Analytics en tiempo real con c谩lculo de ROI
- Integraci贸n plug & play con plataformas de telemedicina existentes

**Datos actuales del sistema:**
- {{patientCount}} pacientes activos
- {{appointmentCount}} consultas programadas
- {{reductionRate}}% reducci贸n de ausentismo lograda

**Tu rol:**
- Responder preguntas sobre la plataforma
- Sugerir acciones espec铆ficas (ej: "crear consulta", "ver pacientes con score bajo")
- Explicar m茅tricas y reportes
- Ayudar con el uso de las funcionalidades

**FORMATO DE RESPUESTAS:**
- NO uses f贸rmulas LaTeX (\[ \], $, etc.)
- Escribe f贸rmulas en texto simple: "ROI = (Beneficio Neto / Costo)  100"
- Usa emojis para hacer las respuestas m谩s amigables: 梆
- Usa saltos de l铆nea y bullets para claridad
- Usa negritas con **texto** para destacar n煤meros importantes

**IMPORTANTE SOBRE CLCULOS DE ROI:**
Cuando el usuario pregunte sobre ahorros econ贸micos, ROI, o cu谩nto dinero puede ahorrar, DEBES usar la funci贸n calculate_roi con los datos que el usuario proporcione o preguntar por los datos faltantes:
- N煤mero de consultas mensuales
- Ingreso que recibe por consulta
- Costo que paga al m茅dico por consulta

NUNCA intentes hacer estos c谩lculos manualmente. SIEMPRE usa la funci贸n calculate_roi que te dar谩 resultados precisos y confiables.

S茅 conciso, profesional y enfocado en ayudar al usuario a usar TeleAssist eficientemente.`;

// Definici贸n de funciones disponibles para el LLM
const FUNCTIONS = [
  {
    name: 'calculate_roi',
    description: 'Calcula el ROI preciso y ahorro econ贸mico al usar TeleAssist. Usa esta funci贸n SIEMPRE que el usuario pregunte sobre ahorros, ROI, beneficios econ贸micos o cu谩nto dinero puede ganar/ahorrar. Devuelve c谩lculos detallados incluyendo ahorro mensual, anual, ROI neto, y periodo de recuperaci贸n.',
    parameters: {
      type: 'object',
      properties: {
        monthlyAppointments: {
          type: 'number',
          description: 'N煤mero total de consultas mensuales programadas (incluye las que asisten y las que no asisten)'
        },
        revenuePerAppointment: {
          type: 'number',
          description: 'Ingreso que recibe el cliente por cada consulta realizada (en d贸lares o pesos)'
        },
        costPerAppointment: {
          type: 'number',
          description: 'Costo que el cliente paga al m茅dico por cada consulta (honorarios m茅dicos en d贸lares o pesos)'
        },
        currentNoShowRate: {
          type: 'number',
          description: 'Tasa actual de ausentismo como decimal (ej: 0.33 para 33%). Si no se proporciona, usa 0.33 (promedio de la industria)',
          default: 0.33
        },
        targetNoShowRate: {
          type: 'number',
          description: 'Tasa objetivo de ausentismo con TeleAssist como decimal (ej: 0.08 para 8%). Si no se proporciona, usa 0.08',
          default: 0.08
        }
      },
      required: ['monthlyAppointments', 'revenuePerAppointment', 'costPerAppointment']
    }
  }
];

/**
 * Ejecuta la funci贸n solicitada por el LLM
 */
function executeFunctionCall(functionName, functionArgs) {
  if (functionName === 'calculate_roi') {
    try {
      const roiData = calculateROI(functionArgs);
      const summary = generateROISummary(roiData);
      
      // Devolver tanto el objeto estructurado como el resumen
      return JSON.stringify({
        success: true,
        data: roiData,
        summary: summary
      });
    } catch (error) {
      return JSON.stringify({
        success: false,
        error: error.message
      });
    }
  }
  
  return JSON.stringify({ success: false, error: 'Funci贸n no reconocida' });
}

/**
 * Handler del endpoint /api/chat
 * Recibe mensajes del usuario y devuelve respuestas contextuales sobre TeleAssist
 */
async function handleChat(req, res) {
  try {
    const { message, conversationHistory = [] } = req.body;

    if (!message || typeof message !== 'string') {
      return res.status(400).json({ 
        error: 'Message is required and must be a string' 
      });
    }

    // Validar que tenemos el token de GitHub
    if (!process.env.GITHUB_TOKEN) {
      return res.status(500).json({ 
        error: 'GitHub token not configured' 
      });
    }

    // Preparar el contexto con datos actuales
    // TODO: En producci贸n, estos datos vendr铆an de la base de datos
    const contextData = {
      patientCount: 500,
      appointmentCount: 520,
      reductionRate: 73
    };

    // Inyectar datos en el system prompt
    let systemPrompt = SYSTEM_PROMPT;
    Object.entries(contextData).forEach(([key, value]) => {
      systemPrompt = systemPrompt.replace(`{{${key}}}`, value);
    });

    // Construir el array de mensajes para la API
    const messages = [
      { role: 'system', content: systemPrompt },
      ...conversationHistory.slice(-10), // ltimos 10 mensajes para no exceder l铆mites
      { role: 'user', content: message }
    ];

    // Llamada inicial a GitHub Models con function calling
    let response = await client.chat.completions.create({
      model: 'gpt-4o-mini', // Modelo High tier: 10 RPM, 50 RPD
      messages: messages,
      functions: FUNCTIONS, // Funciones disponibles
      function_call: 'auto', // El modelo decide cu谩ndo llamar funciones
      temperature: 0.7,
      max_tokens: 800, // Aumentado para respuestas con ROI
      top_p: 0.9
    });

    let assistantMessage = response.choices[0].message;
    let totalUsage = response.usage;

    // Si el LLM quiere llamar a una funci贸n
    if (assistantMessage.function_call) {
      const functionName = assistantMessage.function_call.name;
      const functionArgs = JSON.parse(assistantMessage.function_call.arguments);
      
      console.log(` LLM llamando funci贸n: ${functionName}`, functionArgs);
      
      // Ejecutar la funci贸n
      const functionResult = executeFunctionCall(functionName, functionArgs);
      
      // Agregar el resultado de la funci贸n a la conversaci贸n
      messages.push(assistantMessage); // Mensaje del assistant con function_call
      messages.push({
        role: 'function',
        name: functionName,
        content: functionResult
      });
      
      // Segunda llamada al LLM para que interprete el resultado
      response = await client.chat.completions.create({
        model: 'gpt-4o-mini',
        messages: messages,
        temperature: 0.7,
        max_tokens: 800,
        top_p: 0.9
      });
      
      assistantMessage = response.choices[0].message;
      
      // Sumar usage de ambas llamadas
      totalUsage = {
        prompt_tokens: totalUsage.prompt_tokens + response.usage.prompt_tokens,
        completion_tokens: totalUsage.completion_tokens + response.usage.completion_tokens,
        total_tokens: totalUsage.total_tokens + response.usage.total_tokens
      };
    }

    return res.json({
      message: assistantMessage.content,
      model: 'gpt-4o-mini',
      functionCalled: !!assistantMessage.function_call,
      usage: {
        promptTokens: totalUsage.prompt_tokens,
        completionTokens: totalUsage.completion_tokens,
        totalTokens: totalUsage.total_tokens
      }
    });

  } catch (error) {
    console.error('Error in chat endpoint:', error);

    // Manejo espec铆fico de rate limiting
    if (error.status === 429) {
      return res.status(429).json({ 
        error: 'Rate limit exceeded. Please try again later.',
        retryAfter: error.headers?.['retry-after'] || 60
      });
    }

    // Error gen茅rico
    return res.status(500).json({ 
      error: 'Failed to process chat message',
      details: error.message 
    });
  }
}

/**
 * Health check del servicio de chat
 */
async function healthCheck(req, res) {
  try {
    const hasToken = !!process.env.GITHUB_TOKEN;
    
    return res.json({
      status: 'ok',
      chatEnabled: hasToken,
      model: 'gpt-4o-mini',
      provider: 'GitHub Models',
      functionsAvailable: FUNCTIONS.map(f => f.name)
    });
  } catch (error) {
    return res.status(500).json({ 
      status: 'error',
      error: error.message 
    });
  }
}

module.exports = {
  handleChat,
  healthCheck
};
