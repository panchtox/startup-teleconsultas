# ðŸ’¬ Chat con LLM - TeleAssist

Sistema de chat inteligente que responde preguntas sobre TeleAssist usando GitHub Models (tier gratuito).

## ðŸŽ¯ CaracterÃ­sticas

- **Contextual**: Conoce datos actuales del sistema (pacientes, consultas, mÃ©tricas)
- **Inteligente**: Usa GPT-4o-mini de OpenAI vÃ­a GitHub Models
- **Gratuito**: Free tier con 10 RPM, 50 RPD
- **Sin fricciÃ³n**: Solo necesitas un Personal Access Token de GitHub

## ðŸ“‹ Requisitos

1. Cuenta de GitHub (gratuita)
2. Personal Access Token de GitHub
3. Node.js >= 18.0.0
4. Dependencia `openai` instalada

## ðŸ”§ ConfiguraciÃ³n

### 1. Obtener GitHub Token

1. Ve a https://github.com/settings/tokens
2. Click en "Generate new token (classic)"
3. NO necesitas seleccionar ningÃºn scope
4. Copia el token generado

### 2. Configurar .env

Agrega tu token al archivo `.env`:

```bash
GITHUB_TOKEN=tu_github_personal_access_token_aqui
```

### 3. Instalar dependencias

```bash
npm install
```

## ðŸš€ Uso

### Iniciar el servidor

```bash
npm run dev
```

El chat estarÃ¡ disponible en: `http://localhost:3000/api/chat`

### Endpoint: POST /api/chat

**Request:**
```json
{
  "message": "Â¿CuÃ¡ntos pacientes tenemos activos?",
  "conversationHistory": [
    { "role": "user", "content": "Hola" },
    { "role": "assistant", "content": "Hola! Â¿En quÃ© puedo ayudarte?" }
  ]
}
```

**Response:**
```json
{
  "message": "Actualmente tenemos 500 pacientes activos en el sistema...",
  "model": "gpt-4o-mini",
  "usage": {
    "promptTokens": 150,
    "completionTokens": 50,
    "totalTokens": 200
  }
}
```

### Health Check: GET /api/chat/health

**Response:**
```json
{
  "status": "ok",
  "chatEnabled": true,
  "model": "gpt-4o-mini",
  "provider": "GitHub Models"
}
```

## ðŸ“Š LÃ­mites Free Tier

### GitHub Models - Modelos High (GPT-4o)
- **Requests por minuto**: 10 RPM
- **Requests por dÃ­a**: 50 RPD
- **Tokens input**: 8,000 max
- **Tokens output**: 4,000 max
- **Requests concurrentes**: 2

### Manejo de Rate Limiting

El endpoint responde con `429 Too Many Requests` si se exceden los lÃ­mites:

```json
{
  "error": "Rate limit exceeded. Please try again later.",
  "retryAfter": 60
}
```

## ðŸ§  System Prompt

El asistente tiene contexto sobre:
- **TeleAssist**: Plataforma B2B SaaS para reducir ausentismo
- **Datos actuales**: 500 pacientes, 520 consultas, 73% reducciÃ³n
- **Funcionalidades**: Recordatorios WhatsApp, reputaciÃ³n gamificada, analytics
- **Capacidades**: Responder preguntas, sugerir acciones, explicar mÃ©tricas

## ðŸ”„ MigraciÃ³n a Azure AI (Escalado)

Para escalar a producciÃ³n, solo cambia:

```javascript
const client = new OpenAI({
  baseURL: 'https://YOUR-RESOURCE.openai.azure.com', // <- Solo esto cambia
  apiKey: process.env.AZURE_API_KEY
});
```

El resto del cÃ³digo es compatible sin cambios.

## ðŸ“ Ejemplos de Preguntas

### MÃ©tricas
- "Â¿CuÃ¡ntos pacientes tenemos activos?"
- "Â¿CuÃ¡l es nuestra tasa de reducciÃ³n de ausentismo?"
- "Â¿CuÃ¡ntas consultas hay programadas?"

### Funcionalidades
- "Â¿CÃ³mo funciona el sistema de reputaciÃ³n?"
- "Â¿QuÃ© badges pueden obtener los pacientes?"
- "Â¿CÃ³mo se envÃ­an los recordatorios?"

### Acciones
- "MuÃ©strame pacientes con score bajo"
- "Â¿CÃ³mo crear una nueva consulta?"
- "Â¿CÃ³mo exportar el listado de pacientes?"

## ðŸ› Troubleshooting

### Error: "GitHub token not configured"

**Causa**: Falta `GITHUB_TOKEN` en `.env`

**SoluciÃ³n**:
```bash
# Agregar al .env
GITHUB_TOKEN=ghp_xxxxxxxxxxxxxxxxxxxx
```

### Error 429: Rate limit exceeded

**Causa**: Excediste 10 RPM o 50 RPD

**SoluciÃ³n**:
- Esperar 1 minuto (RPM)
- Esperar hasta el dÃ­a siguiente (RPD)
- Implementar rate limiting en el frontend

### Respuestas incorrectas o desactualizadas

**Causa**: Context data hardcodeado en `chat.js`

**SoluciÃ³n**: Integrar con base de datos real
```javascript
const contextData = {
  patientCount: await db.patients.count(),
  appointmentCount: await db.appointments.count(),
  reductionRate: await calculateReductionRate()
};
```

## ðŸ“š Referencias

- [GitHub Models Docs](https://docs.github.com/en/github-models)
- [Azure AI Inference API](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/develop/inference-api)
- [OpenAI SDK](https://github.com/openai/openai-node)

## ðŸ” Seguridad

- âœ… Usa variables de entorno para tokens
- âœ… Nunca expongas el GITHUB_TOKEN en el frontend
- âœ… Valida inputs antes de enviar al LLM
- âœ… Limita tamaÃ±o de conversationHistory (max 10 mensajes)
- âš ï¸ No envÃ­es datos sensibles de pacientes al LLM en producciÃ³n

## ðŸ“ˆ PrÃ³ximos Pasos

1. **Frontend**: Crear componente React para el chat
2. **Persistencia**: Guardar conversaciones en DB
3. **Context real**: Conectar con datos reales del sistema
4. **Rate limiting**: Implementar lÃ­mite de requests en backend
5. **Streaming**: Agregar respuestas en streaming para mejor UX
6. **Analytics**: Trackear preguntas mÃ¡s comunes

---

**Creado por**: TeleAssist Team  
**Fecha**: Noviembre 2025  
**Status**: âœ… Backend funcional - Pendiente frontend
